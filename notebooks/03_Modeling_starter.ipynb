{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partie 3 : ModÃ©lisation\n",
        "========================\n",
        "\n",
        "Objectifs\n",
        "---------\n",
        "- ImplÃ©menter un modÃ¨le de filtrage collaboratif (User-Based)\n",
        "- ImplÃ©menter un modÃ¨le basÃ© sur le contenu (Content-Based)\n",
        "- ImplÃ©menter un modÃ¨le hybride (ML supervisÃ©)\n",
        "- Comparer les trois approches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chargement des DonnÃ©es\n",
        "----------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DonnÃ©es chargÃ©es : 5000 users, 1000 products, 50000 interactions\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"../data/\"\n",
        "\n",
        "users_df = pd.read_csv(DATA_PATH + 'users_features.csv')\n",
        "products_df = pd.read_csv(DATA_PATH + 'products_features.csv')\n",
        "interactions_df = pd.read_csv(DATA_PATH + 'interactions.csv')\n",
        "\n",
        "print(f\"DonnÃ©es chargÃ©es : {len(users_df)} users, {len(products_df)} products, {len(interactions_df)} interactions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.1 - Filtrage Collaboratif (User-Based)\n",
        "------------------------------------------\n",
        "\n",
        "Recommander les produits apprÃ©ciÃ©s par des utilisateurs similaires.\n",
        "\n",
        "Ã‰tapes :\n",
        "1. CrÃ©er une matrice utilisateur-produit\n",
        "2. Calculer la similaritÃ© entre utilisateurs\n",
        "3. PrÃ©dire les scores pour les produits non vus\n",
        "4. Recommander le top-K produits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrice user-item : (4974, 1000)\n",
            "SparsitÃ© : 99.01%\n"
          ]
        }
      ],
      "source": [
        "# TODO : CrÃ©er la matrice utilisateur-produit\n",
        "\n",
        "\n",
        "user_item_matrix = interactions_df.pivot_table(\n",
        "    index = 'user_id',\n",
        "    columns = 'product_id',\n",
        "    values = 'interaction_type',\n",
        "    aggfunc = 'count',\n",
        "    fill_value = 0\n",
        ")\n",
        "\n",
        "print(f\"Matrice user-item : {user_item_matrix.shape}\")\n",
        "print(f\"SparsitÃ© : {(user_item_matrix == 0).sum().sum() / user_item_matrix.size * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrice de similaritÃ© : (4974, 4974)\n",
            "SimilaritÃ© calculÃ©e\n"
          ]
        }
      ],
      "source": [
        "# TODO : Calculer la similaritÃ© entre utilisateurs\n",
        "\n",
        "user_similarity = cosine_similarity(user_item_matrix)\n",
        "\n",
        "user_similarity = pd.DataFrame(\n",
        "    user_similarity, \n",
        "    index = user_item_matrix.index, \n",
        "    columns = user_item_matrix.index\n",
        ")\n",
        "\n",
        "print(f\"Matrice de similaritÃ© : {user_similarity.shape}\")\n",
        "print(\"SimilaritÃ© calculÃ©e\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommandations pour l'utilisateur 1 :\n",
            "[185, 478, 298, 669, 480]\n"
          ]
        }
      ],
      "source": [
        "# TODO : Fonction de recommandation\n",
        "\n",
        "def recommend_user_based(user_id, user_item_matrix, user_similarity, k_neighbors=10, n_recommendations=5):\n",
        "    \"\"\"\n",
        "    Recommande des produits basÃ©s sur le filtrage collaboratif\n",
        "    \n",
        "    Args:\n",
        "        user_id: ID de l'utilisateur\n",
        "        user_item_matrix: Matrice utilisateur-produit\n",
        "        user_similarity: Matrice de similaritÃ©\n",
        "        k_neighbors: Nombre de voisins Ã  considÃ©rer\n",
        "        n_recommendations: Nombre de recommandations\n",
        "    \n",
        "    Returns:\n",
        "        Liste des product_ids recommandÃ©s\n",
        "    \"\"\"\n",
        "    if user_id not in user_item_matrix.index:\n",
        "        return []\n",
        "\n",
        "    sim_scores = user_similarity.loc[user_id]\n",
        "\n",
        "    sim_scores = sim_scores.drop(user_id)\n",
        "    top_users = sim_scores.sort_values(ascending=False).head(k_neighbors).index\n",
        "\n",
        "    neighbor_items = user_item_matrix.loc[top_users]\n",
        "\n",
        "    weighted_scores = neighbor_items.T.dot(sim_scores[top_users])\n",
        "\n",
        "    user_items = user_item_matrix.loc[user_id]\n",
        "    weighted_scores = weighted_scores[user_items == 0]\n",
        "\n",
        "    recommended_products = weighted_scores.sort_values(ascending=False).head(n_recommendations).index.tolist()\n",
        "\n",
        "    return recommended_products\n",
        "\n",
        "# Test\n",
        "test_user = users_df['user_id'].iloc[0]\n",
        "recommendations_cf = recommend_user_based(test_user, user_item_matrix, user_similarity)\n",
        "print(f\"Recommandations pour l'utilisateur {test_user} :\")\n",
        "print(recommendations_cf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2 - Filtrage par Contenu (Content-Based)\n",
        "--------------------------------------------\n",
        "\n",
        "Recommander des produits similaires Ã  ceux dÃ©jÃ  apprÃ©ciÃ©s.\n",
        "\n",
        "Ã‰tapes :\n",
        "1. CrÃ©er des embeddings de produits (TF-IDF)\n",
        "2. Calculer la similaritÃ© entre produits\n",
        "3. Recommander des produits similaires\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vecteurs TF-IDF : (1000, 89)\n"
          ]
        }
      ],
      "source": [
        "# TODO : CrÃ©er un corpus textuel pour chaque produit\n",
        "\n",
        "products_df['content'] = (\n",
        "    products_df['name'].fillna('').astype(str)\n",
        "     +\n",
        "    ' '\n",
        "     +\n",
        "    products_df['description'].fillna('').astype(str)\n",
        "     +\n",
        "    ' '\n",
        "     +\n",
        "    products_df['category'].fillna('').astype(str)\n",
        ")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
        "product_vectors = tfidf.fit_transform(products_df['content'])\n",
        "\n",
        "print(f\"Vecteurs TF-IDF : {product_vectors.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrice de similaritÃ© produits : (1000, 1000)\n",
            "SimilaritÃ© calculÃ©e\n"
          ]
        }
      ],
      "source": [
        "# TODO : Calculer la similaritÃ© entre produits\n",
        "\n",
        "product_similarity = cosine_similarity(product_vectors)\n",
        "\n",
        "product_similarity = pd.DataFrame(\n",
        "    product_similarity, \n",
        "    index=products_df['product_id'], \n",
        "    columns=products_df['product_id']\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Matrice de similaritÃ© produits : {product_similarity.shape}\")\n",
        "print(\"SimilaritÃ© calculÃ©e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommandations content-based pour l'utilisateur 1 :\n",
            "[540, 2, 285, 341, 348]\n"
          ]
        }
      ],
      "source": [
        "# TODO : Fonction de recommandation content-based\n",
        "\n",
        "def recommend_content_based(user_id, interactions_df, products_df, product_similarity, n_recommendations=5):\n",
        "    \"\"\"\n",
        "    Recommande des produits basÃ©s sur le contenu\n",
        "    \n",
        "    Args:\n",
        "        user_id: ID de l'utilisateur\n",
        "        interactions_df: DataFrame des interactions\n",
        "        products_df: DataFrame des produits\n",
        "        product_similarity: Matrice de similaritÃ©\n",
        "        n_recommendations: Nombre de recommandations\n",
        "    \n",
        "    Returns:\n",
        "        Liste des product_ids recommandÃ©s\n",
        "    \"\"\"\n",
        "    if user_id not in interactions_df['user_id'].unique():\n",
        "        return []\n",
        "\n",
        "    user_products = interactions_df[interactions_df['user_id'] == user_id]['product_id'].unique()\n",
        "\n",
        "    if len(user_products) == 0:\n",
        "        top_products = products_df['product_id'].value_counts().head(n_recommendations).index.tolist()\n",
        "        return top_products\n",
        "\n",
        "    scores = product_similarity.loc[user_products].sum(axis=0)\n",
        "\n",
        "    scores = scores.drop(user_products, errors='ignore')\n",
        "\n",
        "    recommended_products = scores.sort_values(ascending=False).head(n_recommendations).index.tolist()\n",
        "\n",
        "    return recommended_products\n",
        "\n",
        "# Test\n",
        "recommendations_content = recommend_content_based(test_user, interactions_df, products_df, product_similarity)\n",
        "print(f\"Recommandations content-based pour l'utilisateur {test_user} :\")\n",
        "print(recommendations_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.3 - ModÃ¨le Hybride (ML SupervisÃ©)\n",
        "-------------------------------------\n",
        "\n",
        "Combiner features utilisateurs et produits dans un modÃ¨le supervisÃ©.\n",
        "\n",
        "Ã‰tapes :\n",
        "1. CrÃ©er un dataset d'entraÃ®nement (user + product + target)\n",
        "2. EntraÃ®ner un modÃ¨le (RandomForest)\n",
        "3. PrÃ©dire les probabilitÃ©s d'interaction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset d'entraÃ®nement crÃ©Ã© avec succÃ¨s !\n",
            "X_train : (4000, 18)\n",
            "X_test  : (1001, 18)\n",
            "y_train positif : 16033.0 / 4000\n",
            "y_test  positif : 4058.0 / 1001\n"
          ]
        }
      ],
      "source": [
        "# train/test\n",
        "df = interactions_df.merge(users_df, on='user_id', how='left')\n",
        "df = df.merge(products_df, on='product_id', how='left')\n",
        "\n",
        "df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
        "df['added_date'] = pd.to_datetime(df['added_date'])\n",
        "df['interaction_date'] = pd.to_datetime(df['interaction_date'])\n",
        "\n",
        "df = df[df['rating'].notnull()]\n",
        "\n",
        "\n",
        "df['session_duration'].fillna(df['session_duration'].median(), inplace=True)\n",
        "df['avg_rating'].fillna(df['avg_rating'].median(), inplace=True)\n",
        "df['activity'].fillna(df['activity'].median(), inplace=True)\n",
        "\n",
        "features = [\n",
        "    'age', 'gender', 'location', 'activity_level', 'activity', \n",
        "    'avg_price', 'favorite_category', 'days_since_last',\n",
        "    \n",
        "    'category', 'subcategory', 'price', 'stock', 'initial_rating',\n",
        "    'price_range', 'popularity', 'conversion_rate', 'avg_rating',\n",
        "    \n",
        "    'session_duration'\n",
        "]\n",
        "\n",
        "target = 'rating'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\"âœ… Dataset d'entraÃ®nement crÃ©Ã© avec succÃ¨s !\")\n",
        "print(f\"X_train : {X_train.shape}\")\n",
        "print(f\"X_test  : {X_test.shape}\")\n",
        "print(f\"y_train positif : {y_train.sum()} / {len(y_train)}\")\n",
        "print(f\"y_test  positif : {y_test.sum()} / {len(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ModÃ¨le entraÃ®nÃ© !\n",
            "Accuracy : 0.4306\n",
            "ROC AUC  : 0.6345\n",
            "\n",
            "Classification Report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.25      0.02      0.04        52\n",
            "         2.0       0.20      0.04      0.07        50\n",
            "         3.0       0.13      0.05      0.08       133\n",
            "         4.0       0.36      0.46      0.40       323\n",
            "         5.0       0.53      0.61      0.57       443\n",
            "\n",
            "    accuracy                           0.43      1001\n",
            "   macro avg       0.29      0.24      0.23      1001\n",
            "weighted avg       0.39      0.43      0.40      1001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "y_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "if len(set(y_test)) > 2:\n",
        "    roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
        "else:\n",
        "    roc_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
        "\n",
        "print(\"âœ… ModÃ¨le entraÃ®nÃ© !\")\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"ROC AUC  : {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report :\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*comparaison des modÃ¨les*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- pred_label\n- probability\n- true_label\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_pred = \u001b[43mrf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m y_proba = rf_model.predict_proba(X_test)\n\u001b[32m      4\u001b[39m X_test = X_test.copy()  \u001b[38;5;66;03m# Ã©viter les warnings\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alecp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:903\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    885\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    906\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alecp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:945\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    943\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    948\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alecp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alecp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alecp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
            "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- pred_label\n- probability\n- true_label\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf_model.predict(X_test)\n",
        "y_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "X_test = X_test.copy()  # Ã©viter les warnings\n",
        "X_test['true_label'] = y_test.values\n",
        "X_test['pred_label'] = y_pred\n",
        "X_test['probability'] = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba[:, 0]\n",
        "\n",
        "test_users = X_test['user_id'].drop_duplicates().sample(10, random_state=42)\n",
        "\n",
        "results_comparison = []\n",
        "\n",
        "for user_id in test_users:\n",
        "    rec_cf = recommend_user_based(user_id, user_item_matrix, user_similarity, n_recommendations=5)\n",
        "    \n",
        "    rec_content = recommend_content_based(user_id, interactions_df, products_df, product_similarity, n_recommendations=5)\n",
        "    \n",
        "    user_products = X_test[X_test['user_id'] == user_id]\n",
        "    top_rf = (\n",
        "        user_products.sort_values('probability', ascending=False)\n",
        "        .head(5)['product_id']\n",
        "        .tolist()\n",
        "    )\n",
        "    \n",
        "    results_comparison.append({\n",
        "        'user_id': user_id,\n",
        "        'CF_recommendations': rec_cf,\n",
        "        'Content_recommendations': rec_content,\n",
        "        'Model_recommendations': top_rf\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_comparison)\n",
        "print(\"ðŸ”¹ AperÃ§u des recommandations :\")\n",
        "print(results_df.head())\n",
        "\n",
        "def precision_at_k(user_id, recommended, X_test, k=5):\n",
        "    actual = set(X_test[(X_test['user_id'] == user_id) & (X_test['true_label'] == 1)]['product_id'])\n",
        "    if len(actual) == 0:\n",
        "        return np.nan\n",
        "    recommended = set(recommended[:k])\n",
        "    return len(actual & recommended) / k\n",
        "\n",
        "def evaluate_models(results_df, X_test):\n",
        "    precisions = {'CF': [], 'Content': [], 'Model': []}\n",
        "    for _, row in results_df.iterrows():\n",
        "        user_id = row['user_id']\n",
        "        precisions['CF'].append(precision_at_k(user_id, row['CF_recommendations'], X_test))\n",
        "        precisions['Content'].append(precision_at_k(user_id, row['Content_recommendations'], X_test))\n",
        "        precisions['Model'].append(precision_at_k(user_id, row['Model_recommendations'], X_test))\n",
        "    return {m: np.nanmean(v) for m, v in precisions.items()}\n",
        "\n",
        "scores = evaluate_models(results_df, X_test)\n",
        "print(\"\\nðŸ“Š Comparaison des modÃ¨les (Precision@5) :\")\n",
        "for m, s in scores.items():\n",
        "    print(f\"{m:10s} â†’ Precision@5 = {s:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.bar(scores.keys(), scores.values(), color=['#4CAF50', '#2196F3', '#FFC107'])\n",
        "plt.title(\"Comparaison des modÃ¨les de recommandation (Precision@5)\")\n",
        "plt.ylabel(\"Score moyen\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prochaines Ã‰tapes\n",
        "-----------------\n",
        "\n",
        "Passez au notebook 04_Evaluation_starter.ipynb pour Ã©valuer et comparer les trois modÃ¨les avec des mÃ©triques adaptÃ©es.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
